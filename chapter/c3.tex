\chapter{面向异构后端的容器内存压力感知卸载架构总体设计}

本章首先说明系统的设计，然后介绍介绍整体系统的架构、内核态和用户态的各个模块和组件，各个模块之间的交互，以及整个框架的执行流程。

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{架构层次图.pdf}
    \caption{架构层次图}
    \label{fig:system_architecture_hierarchy}
\end{figure}

\section{需要分析与设计目标}

本系统旨在为异构后端提供一个基于内存压力感知的透明卸载的框架，只需要将异构装卸后端接入到基于frontswap接口的swap后端中，就可以实现内存压力感知的透明卸载。有很高的拓展性，无需要每种不同的异构后端实现工作集的计算

该系统采用了一种新颖的内核态-用户态协同方法，以实现对内存压力的精确感知和对容器工作集的动态优化。本研究将内存压力定义为由于内存资源不足而导致的应用程序性能下降，具体表现为同步内存回收操作所占用的时间片比例。基于此，我们首先在内核层构建了一个细粒度的内存压力模型，该模型能够实时、准确地反映系统的内存受限程度；基于该模型，我们在用户态设计了一种自适应的工作集估计算法，该算法能够根据内存压力动态调整容器的内存资源分配。

具体而言，本系统的设计目标涵盖以下功能性需求：

\begin{itemize}
    \item \textbf{内存压力感知：} 内核态模块需要能够精确、实时地监测系统的内存压力，并将压力信息传递给用户态模块。
    \item \textbf{工作集估计与调整：} 用户态模块需要根据内核态提供的内存压力信息，动态估算容器的工作集大小，并据此调整容器的内存资源限制。
    \item \textbf{冷热页优化：} 框架需要优化冷热页面的识别机制，能够根据不同的工作负载特性，动态调整文件页和匿名页的回收比例。
    \item \textbf{透明卸载：} 卸载过程对用户态应用程序透明，无需修改应用程序代码。新的卸载后端设备应能通过标准接口（如扩展 swap 文件系统）接入系统。
\end{itemize}

在非功能性需求方面，主要有以下几点：

\begin{itemize}
    \item \textbf{内存节省：} 框架自身的内存开销应尽可能小，以避免对系统造成额外的内存负担。
    \item \textbf{低计算开销：} 内存压力计算和工作集估计算法的计算复杂度应尽可能低，以减少对 CPU 资源的占用。
\end{itemize}


通过实现上述设计目标，本系统将为容器化环境提供一种高效、灵活、透明的内存压力管理方案，显著提升容器在内存受限场景下的性能和资源利用率。

\section{系统架构}

本论文提出的面向异构后端的容器内存压力感知卸载框架的系统架构如图 \ref{fig:system_architecture_hierarchy} 所示。该框架采用内核态-用户态协同设计模式，旨在实现对容器内存压力的精确感知与动态卸载。图中加粗斜线表示新增模块，加粗部分表示修改模块。

框架底层由异构卸载后端和内核基础能力构成。异构卸载后端支持多种存储设备，包括NVMe SSD、NVM、CXL、RDMA和ZRAM等，可通过扩展swap文件系统实现不同后端的接入。内核基础能力为框架运行提供必要支持，涵盖工作队列、线程管理及并发控制等核心功能。

框架的核心组件为内核态模块，由三个主要子模块构成：内存管理模块、进程管理模块和内存压力模块。内存管理模块包含冷热页面识别、页面分配与回收机制，其中冷热页面识别与页面分配模块已进行优化改进；进程管理模块包含进程管理与调度功能，已实现相应改进；内存压力模块由内存压力计算与内存停顿时间统计组成，均为新增模块。内核态模块深度集成于Linux内核的内存管理子系统，通过扩展页面分配、页面回收及进程调度机制，实现对内存压力的细粒度监测与控制。具体而言，内存压力模块实时计算系统内存压力指标，并通过用户态与内核态通信模块（mpfs）向用户态提供相关信息。进程管理模块则依据内存压力状况，动态调整进程调度策略，以优化系统整体性能。

用户态容器包含三个核心模块：用户负载、cgroup限制模块及工作集大小估算模块。用户负载代表容器内运行的实际应用程序；cgroup限制模块基于Linux内核的cgroup机制，对容器内存资源使用进行约束；工作集大小估算模块根据内核态提供的内存压力信息，动态估算容器工作集大小，并据此调整cgroup内存限制，实现容器内存资源的优化分配。

用户态与内核态之间通过多种通信机制实现交互。除利用cgroupsfs进行cgroup配置外，还可使用标准系统调用接口，如`read`、`write`、`ioctl`和`poll`。此外，通过自定义的`mpfs`（memory pressure file system）接口，可实现更高效、灵活的通信。用户态应用程序可借助这些通信机制获取内核态内存压力信息，并对容器内存资源配置进行动态调整。

本节后续内容将详细阐述系统各模块的功能与职责。


\subsection{内存压力模块}

内存压力模块作为本框架的核心组件，承担着将内核检测到的内存短缺状况进行量化表征的关键任务，为用户态的资源调度与优化决策提供数据支撑。该模块采用动态监测与分析机制，构建了多维度的内存压力评估模型，而非提供单一瞬时压力值。

内存压力模块的核心组件为内存停顿时间统计单元。该单元通过在内核同步内存回收路径上实施精细化插桩，捕获因内存分配请求无法立即满足而触发的同步回收事件。对于每个同步回收事件，系统精确记录其起始时间、终止时间以及期间发生的并发事件（如进程调度、中断处理等），进而计算该回收操作占用的CPU时间片。然后和调度系统交互，得到分配的时间片，最后得到比例。这种时间片比例直接反映了内存子系统处理内存短缺对程序执行造成的延迟影响。值得注意的是，统计范围仅限于同步回收导致的停顿，因为异步回收通常不会直接阻塞应用程序执行。

内存压力计算单元基于内存停顿时间统计单元提供的原始数据，采用加权平均、滑动窗口、指数衰减等统计方法进行多维度分析。该单元不仅考虑最近一次停顿时间，还综合历史数据，生成能够平滑反映内存压力变化的量化指标。该指标设计旨在平衡响应速度与稳定性，既能够及时捕捉内存压力突变，又可避免因短期波动导致的误判，从而为用户态提供可靠的决策依据。

\subsection{通信模块}

通信模块负责实现内核态与用户态之间的高效数据交互，确保内存压力指标能够实时、准确地传递至用户态的容器管理组件。本模块基于proc虚拟文件系统实现，通过创建特定入口（如/proc/mem\_pressure）以标准文件I/O操作方式暴露内存压力信息。该入口采用结构化数据格式（如百分比）呈现内存压力指标，用户态程序可通过read系统调用直接获取相关信息。

为支持用户态程序对内存压力变化的实时响应，通信模块实现了poll系统调用接口。该接口采用事件驱动模型，允许用户态程序注册对特定文件描述符（即内存压力信息入口）的事件监听。当内存压力指标超过预设阈值时，内核通过poll机制主动通知用户态程序，有效避免了频繁轮询带来的性能开销。这种事件驱动设计显著提升了用户态程序响应内存压力变化的及时性与效率。

\subsection{用户态容器}

用户态容器作为应用程序运行的隔离环境，是本框架实现内存压力感知与动态资源调整的执行单元。容器架构包含以下核心组件：

用户负载组件代表容器内运行的实际应用程序或服务。这些应用程序对底层的内存压力感知与资源调整机制保持透明，无需任何修改即可受益于框架提供的性能优化。

工作集大小估算模块是容器的关键组件，负责基于通信模块提供的内存压力信息，动态评估容器的当前工作集大小（即应用程序正常运行所需的最小内存量）。该模块采用启发式算法，综合考虑容器的历史内存使用模式、当前内存压力水平、应用程序类型与行为特征等因素，生成容器的理想内存需求评估结果。

cgroup限制模块利用Linux内核的cgroup机制，对容器的内存资源使用实施动态约束。该模块通过与cgroupsfs文件系统交互，实时调整容器的内存硬限制（memory.limit\_in\_bytes）和软限制（memory.soft\_limit\_in\_bytes）等参数。工作集大小估算模块的输出作为cgroup限制模块的核心输入，指导其设置合理的内存限制，在保障容器性能的同时优化系统资源利用率。

\subsection{内存管理模块}

内存管理模块作为Linux内核的核心组件，负责系统物理内存的分配、回收与维护。本框架重点优化了冷热页面识别机制，提升了内存管理的效率与准确性。
冷热页面识别机制是内存回收和页面置换策略的基础。传统 Linux 内核的内存回收机制在设计时，为了尽可能减少对系统性能的影响，倾向于优先回收文件页（file-backed pages）。这是因为文件页通常可以从磁盘重新读取，而匿名页如果被换出到交换分区（swap），则在重新换入时可能需要更长的延迟。

此外，为支持内存压力模块的统计需求，本框架在内存分配与回收的关键路径上实施了插桩，用于收集分配延迟、回收延迟等详细内存操作信息，为系统性能分析与优化提供了数据支撑。


\section{运行流程}

本节介绍系统运行流程，从框架初始化以及用户态和内核态的交互流程来介绍内存压力模块以及用户态容器组成的完整系统的典型流程，以进一步说明各个模块的之间的关系。


\subsection{框架初始化}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{mp初始化.pdf}
    \caption{框架初始化}
    \label{fig:framework_initialization}
\end{figure}

框架初始化流程是系统构建的基石，为后续的内存压力监控与管理奠定稳固基础。如图 \ref{fig:framework_initialization} 所示，初始化流程主要包含以下几个紧密相连的关键阶段：

首先，系统启动后，会立即进入采样周期与工作队列的配置环节。这一阶段的核心在于为内存压力的周期性监控设定合适的参数。采样周期的选择至关重要，它直接决定了系统对内存压力波动响应的灵敏程度。若采样周期设置过长，则可能错过内存压力的瞬时尖峰，导致无法及时采取应对措施；反之，过短的采样周期会频繁触发数据收集和计算，给系统带来不必要的性能开销。因此，一个合理的采样周期需要在捕捉内存压力变化和降低系统负担之间取得精妙的平衡，这通常需要根据系统的实际负载特性和性能需求进行动态调整。同时，为了避免在中断上下文中执行可能耗时的操作（这会严重影响系统的实时性和稳定性），内核广泛采用工作队列机制来实现任务的异步执行。本框架充分利用了工作队列的优势，将内存压力的统计、计算、以及向用户态报告等一系列相关任务都交由工作队列异步处理。在具体实现上，系统通过内核提供的配置接口（如 sysctl）或模块参数来设置采样周期，并创建一个专用的内核工作队列，将与内存压力处理相关的函数注册到该队列。在每个采样周期结束时，内核会触发工作队列中注册的处理函数，从而启动新一轮的内存压力信息收集与计算流程。

其次，虽然在初始化阶段会为每个 CPU 核心分配并初始化 per-CPU 变量，但这些变量在此时并不会立即投入使用。per-CPU 变量主要用于存储每个 CPU 核心上的内存压力相关统计信息，包括停顿时间、分配延迟等关键指标，以及一些用于辅助计算和同步的元数据。这些 per-CPU 变量的设计充分考虑了数据局部性和访问效率，旨在减少跨 CPU 访问和锁竞争，从而提升系统整体性能。然而，这些变量的初始值仅仅是占位符，其真正发挥作用是在后续的内存回收阶段。在发生内存回收事件时，相关的内核路径会更新对应 CPU 核心上的 per-CPU 变量，记录下详细的停顿时间和其它相关信息。随后，工作队列中的处理函数会定期读取这些 per-CPU 变量，并进行综合分析和计算，最终得出反映系统整体内存压力的指标。这种设计将数据的收集和计算解耦，避免了在内存回收的关键路径上引入过多的性能开销。

最后，系统会着手初始化并挂载 mpfs（Memory Pressure File System）文件系统。mpfs 作为本框架实现内核态与用户态高效通信的核心组件，其本质是一个驻留在内存中的虚拟文件系统，不涉及任何磁盘 I/O 操作，从而确保了极低的访问延迟。通过调用内核提供的 mount 系统调用，mpfs 文件系统会被挂载到预先指定的目录（通常为 /sys/fs/mpfs）。在挂载过程中，内核会创建一系列内部数据结构来表示 mpfs 文件系统，并将其与 VFS（Virtual File System）层紧密集成。与此同时，mpfs 文件系统自身的一些元数据信息，如 inode 信息、文件操作函数指针等，也会在这一阶段被初始化。这些元数据信息定义了 mpfs 文件系统的行为和特性，使其能够正确响应用户态的文件操作请求。通过 mpfs，内核可以将内存压力信息以标准文件的形式暴露给用户态，而用户态程序可以通过常规的文件操作（如 read、poll 等）便捷地获取这些信息，从而实现对系统内存压力的实时监控和响应。


\subsection{容器环境下面向异构装卸后端内存优化框架执行流程}
\label{sec:容器环境下面向异构装卸后端内存优化框架执行流程}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth,keepaspectratio]{内核态序列图.pdf}
\caption{异构内存管理框架内核态交互序列图}
\label{fig:kernel_sequence_diagram_1}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth,keepaspectratio]{用户态序列图.pdf}
\caption{异构内存管理框架用户态交互序列图}
\label{fig:kernel_sequence_diagram_2}
\end{figure}

本节主要说明用户态与内核态协同工作，将内存资源卸载到异构后端的流程。该机制主要包以下两个核心控制流程：

\begin{itemize}
\item 内存扩容流程：当用户态内存使用量增加时，触发内存同步回收机制，将访问频率较的内存页面迁移至异构存储后端。同时，系统记录迁移过程的停顿时间，并将内存压力指标递至用户态。用户态工作集估算模块根据压力指标计算最优工作集大小，并通过cgroup机制态调整内存限额，从而确保应用程序性能满足服务水平目标(SLO)要求。
\item 内存收缩流程：当用户态内存使用量降低，未触发内存同步回收时，系统检测到内存力下降。工作集估算模块据此判定容器处于资源闲置状态，缩小工作集，随后通过cgroup机制收紧内存额，以提升资源利用效率。
\end{itemize}

图\ref{fig:kernel_sequence_diagram_1}和图\ref{fig:kernel_sequence_diagram_2}详细展示了各个模块之间的交互流程。

\begin{itemize}
    \item \textbf{1. 内核态内存压力监测与计算}
    \begin{itemize}
        \item[\textbf{(a)}] \textbf{内存同步回收事件触发：} 当应用程序发起内存分配请求，而系统无法立即满足该请求时，将触发内存同步回收过程。此时，内存停顿时间统计模块立即介入，记录该同步回收事件的起始时间、相关的状态变更信息（例如，涉及的进程、内存区域等），并启动对整个同步回收过程的持续跟踪。
        \item[\textbf{(b)}] \textbf{同步回收过程监测：} 在同步回收过程中，系统可能会发生进程切换（例如，由于时间片耗尽或更高优先级进程抢占）或时钟中断。内存停顿时间统计模块会精确捕获这些事件，并记录相应的时间点，以便后续分析这些事件对同步回收延迟的影响。
        \item[\textbf{(c)}] \textbf{同步回收结束处理：} 当同步内存回收过程结束（即，内存分配请求最终得到满足或因其他原因终止）时，内存停顿时间统计模块停止统计，以及状态变更。
        \item[\textbf{(d)}] \textbf{周期性统计与计算：} 内存压力计算模块以固定的时间间隔（本系统中为每两秒）进行一次统计。它首先从内存停顿时间统计模块获取在过去一段时间内记录的所有时间片。然后，内存压力计算模块运用统计方法（如加权平均、滑动窗口、指数衰减等）对这些原始数据进行处理，计算出一个能够反映当前系统内存压力的综合指标，最终生成一个量化的内存压力值。
        \item[\textbf{(e)}] \textbf{内存压力数据传递：} 计算得到的内存压力指标通过 `mpfs`（内存压力文件系统）接口暴露给用户态。`mpfs` 是一个自定义的虚拟文件系统，它提供了一种标准的文件 I/O 接口，使得用户态进程可以像读取普通文件一样读取内核态的内存压力数据。
    \end{itemize}

    \item \textbf{2. 用户态容器工作集评估与资源调整}
    \begin{itemize}
        \item[\textbf{(a)}] \textbf{内存压力数据请求：} 用户态的容器中，工作集大小估计模块通过 `mpfs` 接口，向内核态的内存压力模块发起内存压力数据请求。这通常是一个阻塞式的读取操作，如果当前没有新的内存压力数据可用，工作集大小估计模块将进入等待状态，直到有新的数据到达。
        \item[\textbf{(b)}] \textbf{内存压力状态反馈：} 内核态的内存压力模块通过 `mpfs` 接口，将最新的内存压力状态反馈给用户态的工作集大小估计模块。
        \item[\textbf{(c)}] \textbf{工作集估算：} 工作集大小估计模块接收到内存压力数据后，根据该数据以及容器的历史内存使用信息、应用程序的类型和行为特征等因素，运行特定的工作集估计算法。该算法的目标是估算出容器当前正常运行所需的最小内存量，即容器的工作集大小。
        \item[\textbf{(d)}] \textbf{内存限制设置：} 工作集大小估计模块将估算出的容器工作集大小（以内存需求量的形式）传递给 cgroup 限制模块。cgroup 限制模块是 Linux 内核 cgroup 机制的一个用户态接口，它负责根据接收到的内存需求量，通过 cgroupfs 文件系统接口，设置容器的内存资源限制值（如 memory.limit\_in\_bytes 和 memory.soft\_limit\_in\_bytes）。
        \item[\textbf{(e)}] \textbf{配置确认与执行：} cgroupfs 文件系统接收到 cgroup 限制模块的配置请求后，执行相应的配置操作，并将配置结果（成功或失败）返回给 cgroup 限制模块。cgroup 限制模块确认配置成功后，开始执行新的内存限制策略。
        \item[\textbf{(f)}] \textbf{限制确认：} cgroup 内核模块将内存限制的执行状态（例如，是否成功应用了新的限制）反馈给用户态的 cgroup 限制模块。
    \end{itemize}

    \item \textbf{3. 内存分配、回收与卸载}
    \begin{itemize}
        \item[\textbf{(g)}] \textbf{内存分配与回收：}
        \begin{itemize}
            \item  当`cgroup`限制模块对容器的内存分配施加了限制后，如果容器内的应用程序请求的内存超过了可用资源，将触发内核的内存回收机制。
            \item 内存回收机制启动后，会首先触发冷热页面识别过程。该过程旨在区分出容器内存中哪些页面是经常访问的（热页），哪些页面是较少访问的（冷页）。
            \item 如果在内存回收过程中发生了内存同步回收（即，由于内存不足，回收过程阻塞了应用程序的执行），则会再次进入内存停顿时间统计模块，重复步骤 1(a) 的流程，对本次同步回收事件进行详细的记录和分析。
        \end{itemize}
        \item[\textbf{(h)}] \textbf{基于frontswap的卸载：}
        \begin{itemize}
            \item 冷热页面识别模块完成页面冷热属性的识别后，将识别结果（即哪些页面是冷页）告知内存回收机制。
            \item 系统根据冷热页面识别的结果，利用 frontswap 接口，将识别出的冷页从物理内存中卸载到预先配置好的异构后端存储设备（如 NVMe SSD、NVM、CXL 设备等）。frontswap 是一个内核接口，它提供了一种透明的页面卸载机制，可以将内存中的页面转移到其他存储设备。
            \item 当冷页成功卸载到异构后端存储设备后，frontswap 接口会向内存回收机制发送卸载成功的反馈信息。
            \item 如果在内存回收过程中没有触发内存同步回收（即，回收过程没有阻塞应用程序的执行），并且内存回收操作成功释放了足够的内存空间，那么系统将再次尝试满足应用程序的内存分配请求。
        \end{itemize}
    \end{itemize}
\end{itemize}


\section{本章小结}


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth,keepaspectratio]{架构图.pdf}
    \caption{系统整体架构}
    \label{fig:system_architecture}
\end{figure}

本章主要介绍了容器环境下面向异构装卸后端的分层内存的框架的总体架构，以及各个模块，介绍了框架的初始化流程，以及用户态与内核态协同的异构内存资源动态管理流程。他整个架构图如图\ref{fig:system_architecture}所示。

本系统采用层次化设计，横跨用户空间、内核空间以及硬件层。在容器内部，运行着用户的工作负载，其产生的内存访问请求由内核的内存管理子系统进行处理。

用户空间部署了两个关键组件：工作集估算器（Working Set Estimator, WSE）和 cgroup 管理器。WSE 扮演着感知、决策和执行的角色，它通过 /proc 文件系统等机制持续地从内核态的内存压力监控器获取实时的内存压力信息。不同于传统的周期性轮询，WSE 还支持基于事件的通知机制，以实现更及时的响应。

内核空间的核心组件是内存压力监控器（Pressure Monitoring, PM）。PM 并非简单的统计模块，而是一个深度嵌入到内核内存管理子系统中的主动监控与分析单元，它与内存分配、页面回收等关键路径紧密集成。具体而言，PM 采用类似于静态插桩的技术，在每次发生同步内存回收时，记录本次同步中断所占用的时间片，并与调度系统交互以计算回收延迟，生成一个量化的压力信号（例如，数值或等级），该信号通过 /proc 文件系统周期性地暴露给用户空间。此外，为了减少用户空间轮询的开销，PM 还实现了基于事件的通知机制，当内存压力超过预设阈值时，主动通知用户空间的工作集估算器。

本系统的核心在于其负反馈调节机制，该机制通过内核态与用户态的紧密协同，实现对容器内存资源的动态、精细化管理。内核态的PM持续监测系统的内存使用状况，并生成量化的压力信号。该信号通过 /proc 文件系统等机制周期性地暴露给用户空间，同时支持基于事件的通知机制，以便在压力变化时及时通知用户空间的WSE。当系统内存压力较低时，表明内存资源相对充裕。此时，WSE 会指示 cgroup 管理器收缩容器的内存分配上限。这一操作的目的在于主动触发内核的页面回收机制。由于容器的内存使用受到更严格的限制，内核会更积极地回收不活跃的冷页面，将它们换出到分层存储上。这种策略的核心思想是"预先释放，主动应对"，避免了在内存压力突然增大时才被动地进行页面回收，从而减少了应用程序因内存回收而产生的延迟。当系统整体内存压力升高时（例如，由于其他容器的内存需求增加），PM 会迅速检测到这一变化，并更新其输出的压力信号（例如，提高压力等级或触发事件通知）。WSE 接收到升高的压力信号后，会触发负反馈调节。它会重新评估当前容器的内存需求，并根据预定义的策略（这些策略会综合考虑容器的优先级、历史内存使用情况、服务质量要求等因素，并将在后续章节详细阐述）计算出一个更大的工作集大小。随后，WSE 通过与 cgroup 管理器的交互，将新的工作集大小转化为对 cgroup v2 内存限制参数的具体设置。cgroup 管理器通过调整 cgroup v2 提供的接口（例如 memory.high, memory.max 等），放宽对容器内存使用量的限制，允许容器保留更大的工作集。这样做可以减少因内存不足而导致的缺页中断，从而保障应用程序的性能。

上述过程形成了一个完整的负反馈闭环：内核态的 PM 提供实时的压力反馈（作为反馈信号），用户态的 WSE 根据压力信号进行决策（作为控制器），cgroup 管理器执行资源控制（作为执行器），容器的工作负载在调整后的资源约束下运行（作为被控对象）。容器的内存访问行为会进一步影响内核的内存压力，从而形成一个持续的、自适应的动态调节循环。这种内核态与用户态紧密协同的负反馈机制，使得系统能够灵敏、准确地响应内存压力的变化，在保证关键应用性能的同时，最大化异构内存系统的整体利用率，并有效避免了因内存过度分配导致的系统抖动和性能下降。此外，由于采用了更细粒度的压力信号和更及时的响应机制，本系统相比传统的基于粗粒度统计信息（如缺页中断次数）的方案，具有更高的稳定性和鲁棒性。
