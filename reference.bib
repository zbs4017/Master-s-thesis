
@article{wang1999sanwei,
  title   = {三维矢量散射积分方程中奇异性分析},
  author  = {王浩刚 and 聂在平},
  journal = {电子学报},
  volume  = {27},
  number  = {12},
  pages   = {68 -- 71},
  year    = {1999}
}
@article{jiang2002lirs,
  author     = {Jiang, Song and Zhang, Xiaodong},
  title      = {LIRS: An Efficient Low Inter-Reference Recency Set Replacement Policy to Improve Buffer Cache Performance},
  journal    = {SIGMETRICS Perform. Eval. Rev.},
  volume     = {30},
  number     = {1},
  pages      = {31--42},
  year       = {2002},
  issn       = {0163-5999},
  url        = {https://dl.acm.org/doi/10.1145/511399.511340},
  urldate    = {2025-02-21},
  shorttitle = {LIRS}
}
@article{10.1145/2318857.2254766,
  author     = {Atikoglu, Berk and Xu, Yuehai and Frachtenberg, Eitan and Jiang, Song and Paleczny, Mike},
  title      = {Workload analysis of a large-scale key-value store},
  year       = {2012},
  issue_date = {June 2012},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {40},
  number     = {1},
  issn       = {0163-5999},
  url        = {https://doi.org/10.1145/2318857.2254766},
  doi        = {10.1145/2318857.2254766},
  abstract   = {Key-value stores are a vital component in many scale-out enterprises, including social networks, online retail, and risk analysis. Accordingly, they are receiving increased attention from the research community in an effort to improve their performance, scalability, reliability, cost, and power consumption. To be effective, such efforts require a detailed understanding of realistic key-value workloads. And yet little is known about these workloads outside of the companies that operate them. This paper aims to address this gap.To this end, we have collected detailed traces from Facebook's Memcached deployment, arguably the world's largest. The traces capture over 284 billion requests from five different Memcached use cases over several days. We analyze the workloads from multiple angles, including: request composition, size, and rate; cache efficacy; temporal patterns; and application use cases. We also propose a simple model of the most representative trace to enable the generation of more realistic synthetic workloads by the community.Our analysis details many characteristics of the caching workload. It also reveals a number of surprises: a GET/SET ratio of 30:1 that is higher than assumed in the literature; some applications of Memcached behave more like persistent storage than a cache; strong locality metrics, such as keys accessed many millions of times a day, do not always suffice for a high hit rate; and there is still room for efficiency and hit rate improvements in Memcached's implementation. Toward the last point, we make several suggestions that address the exposed deficiencies.},
  journal    = {SIGMETRICS Perform. Eval. Rev.},
  month      = jun,
  pages      = {53–64},
  numpages   = {12},
  keywords   = {key-value store, memcached, workload analysis, workload modeling}
}
@article{9242282,
  author   = {Jiang, Congfeng and Qiu, Yitao and Shi, Weisong and Ge, Zhefeng and Wang, Jiwei and Chen, Shenglei and Cérin, Christophe and Ren, Zujie and Xu, Guoyao and Lin, Jiangbin},
  journal  = {IEEE Transactions on Cloud Computing},
  title    = {Characterizing Co-Located Workloads in Alibaba Cloud Datacenters},
  year     = {2022},
  volume   = {10},
  number   = {4},
  pages    = {2381-2397},
  keywords = {Containers;Data centers;Cloud computing;Task analysis;Job shop scheduling;Resource management;Co-located jobs;workload characterization;online services;batch jobs;Internet data center;scheduling},
  doi      = {10.1109/TCC.2020.3034500}
}

@inproceedings{atikoglu2012workload,
  author    = {Atikoglu, Berk and Xu, Yuehai and Frachtenberg, Eitan and Jiang, Song and Paleczny, Mike},
  title     = {Workload analysis of a large-scale key-value store},
  year      = {2012},
  isbn      = {9781450310970},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2254756.2254766},
  doi       = {10.1145/2254756.2254766},
  abstract  = {Key-value stores are a vital component in many scale-out enterprises, including social networks, online retail, and risk analysis. Accordingly, they are receiving increased attention from the research community in an effort to improve their performance, scalability, reliability, cost, and power consumption. To be effective, such efforts require a detailed understanding of realistic key-value workloads. And yet little is known about these workloads outside of the companies that operate them. This paper aims to address this gap.To this end, we have collected detailed traces from Facebook's Memcached deployment, arguably the world's largest. The traces capture over 284 billion requests from five different Memcached use cases over several days. We analyze the workloads from multiple angles, including: request composition, size, and rate; cache efficacy; temporal patterns; and application use cases. We also propose a simple model of the most representative trace to enable the generation of more realistic synthetic workloads by the community.Our analysis details many characteristics of the caching workload. It also reveals a number of surprises: a GET/SET ratio of 30:1 that is higher than assumed in the literature; some applications of Memcached behave more like persistent storage than a cache; strong locality metrics, such as keys accessed many millions of times a day, do not always suffice for a high hit rate; and there is still room for efficiency and hit rate improvements in Memcached's implementation. Toward the last point, we make several suggestions that address the exposed deficiencies.},
  booktitle = {Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
  pages     = {53–64},
  numpages  = {12},
  keywords  = {workload modeling, workload analysis, memcached, key-value store},
  location  = {London, England, UK},
  series    = {SIGMETRICS '12}
}


@article{Stonebraker2013TheVM,
  title   = {The VoltDB Main Memory DBMS},
  author  = {Michael Stonebraker and Ariel Weisberg},
  journal = {IEEE Data Eng. Bull.},
  year    = {2013},
  volume  = {36},
  pages   = {21-27},
  url     = {https://api.semanticscholar.org/CorpusID:6306329}
}
@article{fitzpatrick2004distributed,
  author     = {Fitzpatrick, Brad},
  title      = {Distributed caching with memcached},
  year       = {2004},
  issue_date = {August 2004},
  publisher  = {Belltown Media},
  address    = {Houston, TX},
  volume     = {2004},
  number     = {124},
  issn       = {1075-3583},
  abstract   = {Speed up your database app with a simple, fast caching layer that uses your existing servers' spare memory.},
  journal    = {Linux J.},
  month      = aug,
  pages      = {5}
}
@article{sakr2021future,
  author     = {Sakr, Sherif and Bonifati, Angela and Voigt, Hannes and Iosup, Alexandru and Ammar, Khaled and Angles, Renzo and Aref, Walid and Arenas, Marcelo and Besta, Maciej and Boncz, Peter A. and Daudjee, Khuzaima and Valle, Emanuele Della and Dumbrava, Stefania and Hartig, Olaf and Haslhofer, Bernhard and Hegeman, Tim and Hidders, Jan and Hose, Katja and Iamnitchi, Adriana and Kalavri, Vasiliki and Kapp, Hugo and Martens, Wim and \"{O}zsu, M. Tamer and Peukert, Eric and Plantikow, Stefan and Ragab, Mohamed and Ripeanu, Matei R. and Salihoglu, Semih and Schulz, Christian and Selmer, Petra and Sequeda, Juan F. and Shinavier, Joshua and Sz\'{a}rnyas, G\'{a}bor and Tommasini, Riccardo and Tumeo, Antonino and Uta, Alexandru and Varbanescu, Ana Lucia and Wu, Hsiang-Yun and Yakovets, Nikolay and Yan, Da and Yoneki, Eiko},
  title      = {The future is big graphs: a community view on graph processing systems},
  year       = {2021},
  issue_date = {September 2021},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {64},
  number     = {9},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/3434642},
  doi        = {10.1145/3434642},
  abstract   = {Ensuring the success of big graph processing for the next decade and beyond.},
  journal    = {Commun. ACM},
  month      = aug,
  pages      = {62–71},
  numpages   = {10}
}
@inproceedings{gonzalez2014graphx,
  author    = {Gonzalez, Joseph E. and Xin, Reynold S. and Dave, Ankur and Crankshaw, Daniel and Franklin, Michael J. and Stoica, Ion},
  title     = {GraphX: graph processing in a distributed dataflow framework},
  year      = {2014},
  isbn      = {9781931971164},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {In pursuit of graph processing performance, the systems community has largely abandoned general-purpose distributed dataflow frameworks in favor of specialized graph processing systems that provide tailored programming abstractions and accelerate the execution of iterative graph algorithms. In this paper we argue that many of the advantages of specialized graph processing systems can be recovered in a modern general-purpose distributed dataflow system. We introduce GraphX, an embedded graph processing framework built on top of Apache Spark, a widely used distributed dataflow system. GraphX presents a familiar composable graph abstraction that is sufficient to express existing graph APIs, yet can be implemented using only a few basic dataflow operators (e.g., join, map, group-by). To achieve performance parity with specialized graph systems, GraphX recasts graph-specific optimizations as distributed join optimizations and materialized view maintenance. By leveraging advances in distributed dataflow frameworks, GraphX brings low-cost fault tolerance to graph processing. We evaluate GraphX on real workloads and demonstrate that GraphX achieves an order of magnitude performance gain over the base dataflow framework and matches the performance of specialized graph processing systems while enabling a wider range of computation.},
  booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {599–613},
  numpages  = {15},
  location  = {Broomfield, CO},
  series    = {OSDI'14}
}
@inproceedings{rajbhandari2021zero,
  author    = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  title     = {ZeRO-infinity: breaking the GPU memory wall for extreme scale deep learning},
  year      = {2021},
  isbn      = {9781450384421},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3458817.3476205},
  doi       = {10.1145/3458817.3476205},
  abstract  = {In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model.In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs (40\% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed 1.},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  articleno = {59},
  numpages  = {14},
  location  = {St. Louis, Missouri},
  series    = {SC '21},
  pages     = {59-74}
}
@inproceedings{lin2012large,
  author    = {Lin, Jimmy and Kolcz, Alek},
  title     = {Large-scale machine learning at twitter},
  year      = {2012},
  isbn      = {9781450312479},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2213836.2213958},
  doi       = {10.1145/2213836.2213958},
  abstract  = {The success of data-driven solutions to difficult problems, along with the dropping costs of storing and processing massive amounts of data, has led to growing interest in large-scale machine learning. This paper presents a case study of Twitter's integration of machine learning tools into its existing Hadoop-based, Pig-centric analytics platform. We begin with an overview of this platform, which handles "traditional" data warehousing and business intelligence tasks for the organization. The core of this work lies in recent Pig extensions to provide predictive analytics capabilities that incorporate machine learning, focused specifically on supervised classification. In particular, we have identified stochastic gradient descent techniques for online learning and ensemble methods as being highly amenable to scaling out to large amounts of data. In our deployed solution, common machine learning tasks such as data sampling, feature generation, training, and testing can be accomplished directly in Pig, via carefully crafted loaders, storage functions, and user-defined functions. This means that machine learning is just another Pig script, which allows seamless integration with existing infrastructure for data management, scheduling, and monitoring in a production environment, as well as access to rich libraries of user-defined functions and the materialized output of other scripts.},
  booktitle = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
  pages     = {793–804},
  numpages  = {12},
  keywords  = {ensembles, logistic regression, online learning, stochastic gradient descent},
  location  = {Scottsdale, Arizona, USA},
  series    = {SIGMOD '12}
}
@inproceedings{abadi2016tensorflow,
  author    = {Abadi, Mart\'{\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  title     = {TensorFlow: a system for large-scale machine learning},
  year      = {2016},
  isbn      = {9781931971331},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
  booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {265–283},
  numpages  = {19},
  location  = {Savannah, GA, USA},
  series    = {OSDI'16}
}
@inproceedings{7838026,
  author    = {Lee, Seok-Hee},
  booktitle = {2016 IEEE International Electron Devices Meeting (IEDM)},
  title     = {Technology scaling challenges and opportunities of memory devices},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {1.1.1-1.1.8},
  keywords  = {Three-dimensional displays;Performance evaluation;Computer architecture;Transistors;Microprocessors;Phase change random access memory},
  doi       = {10.1109/IEDM.2016.7838026}
}

@phdthesis{李健2015非易失性存储器的能耗研究,
  title       = {非易失性存储器的能耗研究},
  author      = {李健},
  institution = {重庆大学},
  year        = {2015},
  pages       = {1-100},
  address     = {重庆}
}
@inproceedings{inproceedings,
  author  = {Chen, Shimin and Gibbons, Phillip and Nath, Suman},
  year    = {2011},
  month   = {04},
  pages   = {21-31},
  title   = {Rethinking Database Algorithms for Phase Change Memory},
  journal = {CIDR 2011 - 5th Biennial Conference on Innovative Data Systems Research, Conference Proceedings}
}
@inproceedings{jiang2005clockpro,
  author    = {Jiang, Song and Chen, Feng and Zhang, Xiaodong},
  title     = {CLOCK-Pro: an effective improvement of the CLOCK replacement},
  year      = {2005},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {With the ever-growing performance gap between memory systems and disks, and rapidly improving CPU performance, virtual memory (VM) management becomes increasingly important for overall system performance. However, one of its critical components, the page replacement policy, is still dominated by CLOCK, a replacement policy developed almost 40 years ago. While pure LRU has an unaffordable cost in VM, CLOCK simulates the LRU replacement algorithm with a low cost acceptable in VM management. Over the last three decades, the inability of LRU as well as CLOCK to handle weak locality accesses has become increasingly serious, and an effective fix becomes increasingly desirable.Inspired by our I/O buffer cache replacement algorithm, LIRS [13], we propose an improved CLOCK replacement policy, called CLOCK-Pro. By additionally keeping track of a limited number of replaced pages, CLOCK-Pro works in a similar fashion as CLOCK with a VM-affordable cost. Furthermore, it brings all the much-needed performance advantages from LIRS into CLOCK. Measurements from an implementation of CLOCK-Pro in Linux Kernel 2.4.21 show that the execution times of some commonly used programs can be reduced by up to 47\%.},
  booktitle = {Proceedings of the Annual Conference on USENIX Annual Technical Conference},
  pages     = {35},
  numpages  = {1},
  location  = {Anaheim, CA},
  series    = {ATEC '05}
}

@inproceedings{yoon2021dilos,
  author     = {Yoon, Wonsup and Oh, Jinyoung and Ok, Jisu and Moon, Sue and Kwon, Youngjin},
  title      = {DiLOS: Adding Performance to Paging-Based Memory Disaggregation},
  pages      = {70--78},
  year       = {2021},
  url        = {https://dl.acm.org/doi/10.1145/3476886.3477507},
  urldate    = {2025-02-17},
  shorttitle = {DiLOS},
  booktitle  = {Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems},
  series     = {APSys '21},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  isbn       = {978-1-4503-8698-2},
  language   = {en}
}
@article{riley2017deriving,
  author  = {Riley, Richard and Ensor, Joie and Jackson, Dan and Burke, Danielle},
  year    = {2017},
  month   = {02},
  pages   = {096228021668803},
  title   = {Deriving percentage study weights in multi-parameter meta-analysis models: with application to meta-regression, network meta-analysis and one-stage individual participant data models},
  volume  = {27},
  journal = {Statistical Methods in Medical Research},
  doi     = {10.1177/0962280216688033}
}
@inproceedings{petrucci2012lucky,
  author    = {Petrucci, Vinicius and Loques, Orlando and Moss\'{e}, Daniel},
  title     = {Lucky scheduling for energy-efficient heterogeneous multi-core systems},
  year      = {2012},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {Heterogeneous multi-core processors with big/high-performance and small/low-power cores have been proposed as an alternative design to improve energy efficiency over traditional homogeneous multi-cores. We make the case for proportional-share scheduling of threads in heterogeneous processor cores aimed at improving combined energy efficiency and performance. Our thread scheduling algorithm, lucky, is based on lottery scheduling and has been implemented using Linux performance monitoring and thread-to-core affinity capabilities at user-level. Our preliminary results show that lucky scheduling can provide better performance and energy savings over state-of-the-art heterogeneous-aware scheduling techniques.},
  booktitle = {Proceedings of the 2012 USENIX Conference on Power-Aware Computing and Systems},
  pages     = {7},
  numpages  = {1},
  location  = {Hollywood, CA},
  series    = {HotPower'12}
}
@inproceedings{Hwang1981MultipleAD,
  title     = {Multiple Attribute Decision Making: Methods and Applications - A State-of-the-Art Survey},
  author    = {Ching-Lai Hwang and Kwangsun Yoon},
  booktitle = {Lecture notes in economics and mathematical systems},
  year      = {1981},
  url       = {https://api.semanticscholar.org/CorpusID:63825953},
  pages     = {8}
}
@article{david2017scheduling,
  author = {David, Alan},
  year   = {2017},
  month  = {02},
  pages  = {},
  title  = {Scheduling Algorithms for Asymmetric Multi-core Processors},
  doi    = {10.48550/arXiv.1702.04028}
}
@article{gardner1985exponential,
  author   = {Gardner Jr., Everette S.},
  title    = {Exponential smoothing: The state of the art},
  journal  = {Journal of Forecasting},
  volume   = {4},
  number   = {1},
  pages    = {1-28},
  keywords = {Bibliography—exponential smoothing, Comparative methods—ARIMA, exponential smoothing, Control charts—CUSUM, Evaluation—forecast monitoring systems, exponential smoothing, adaptive, Exponential smoothing—adaptive, coefficient choice, higher-order, review, theory, Seasonality—estimation, harmonics Tracking signal—methodology, Use—inventory control},
  doi      = {https://doi.org/10.1002/for.3980040103},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980040103},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.3980040103},
  abstract = {Abstract This paper is a critical review of exponential smoothing since the original work by Brown and Holt in the 1950s. Exponential smoothing is based on a pragmatic approach to forecasting which is shared in this review. The aim is to develop state-of-the-art guidelines for application of the exponential smoothing methodology. The first part of the paper discusses the class of relatively simple models which rely on the Holt-Winters procedure for seasonal adjustment of the data. Next, we review general exponential smoothing (GES), which uses Fourier functions of time to model seasonality. The research is reviewed according to the following questions. What are the useful properties of these models? What parameters should be used? How should the models be initialized? After the review of model-building, we turn to problems in the maintenance of forecasting systems based on exponential smoothing. Topics in the maintenance area include the use of quality control models to detect bias in the forecast errors, adaptive parameters to improve the response to structural changes in the time series, and two-stage forecasting, whereby we use a model of the errors or some other model of the data to improve our initial forecasts. Some of the major conclusions: the parameter ranges and starting values typically used in practice are arbitrary and may detract from accuracy. The empirical evidence favours Holt's model for trends over that of Brown. A linear trend should be damped at long horizons. The empirical evidence favours the Holt-Winters approach to seasonal data over GES. It is difficult to justify GES in standard form–the equivalent ARIMA model is simpler and more efficient. The cumulative sum of the errors appears to be the most practical forecast monitoring device. There is no evidence that adaptive parameters improve forecast accuracy. In fact, the reverse may be true.},
  year     = {1985}
}

@inproceedings{ruan2020aifm,
  author     = {Ruan, Zhenyuan and Schwarzkopf, Malte and Aguilera, Marcos K. and Belay, Adam},
  title      = {\{AIFM\}: \{high-Performance\}, \{application-Integrated\} Far Memory},
  pages      = {315--332},
  year       = {2020},
  url        = {https://www.usenix.org/conference/osdi20/presentation/ruan},
  urldate    = {2025-02-18},
  shorttitle = {\{AIFM\}},
  isbn       = {978-1-939133-19-9},
  language   = {en},
  booktitle  = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)}
}

@inproceedings{yoon2023dilos,
  author     = {Yoon, Wonsup and Ok, Jisu and Oh, Jinyoung and Moon, Sue and Kwon, Youngjin},
  title      = {DiLOS: Do Not Trade Compatibility for Performance in Memory Disaggregation},
  pages      = {266--282},
  year       = {2023},
  url        = {https://doi.org/10.1145/3552326.3567488},
  urldate    = {2025-02-17},
  shorttitle = {DiLOS},
  booktitle  = {Proceedings of the Eighteenth European Conference on Computer Systems},
  series     = {EuroSys '23},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  isbn       = {978-1-4503-9487-1}
}

@inproceedings{10.1145/3620666.3651323,
  author    = {Gao, Ruihao and Li, Zhichun and Tan, Guangming and Li, Xueqi},
  title     = {BeeZip: Towards An Organized and Scalable Architecture for Data Compression},
  year      = {2024},
  isbn      = {9798400703867},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3620666.3651323},
  doi       = {10.1145/3620666.3651323},
  abstract  = {Data compression plays a critical role in operating systems and large-scale computing workloads. Its primary objective is to reduce network bandwidth consumption and memory/storage capacity utilization. Given the need to manipulate hash tables, and execute matching operations on extensive data volumes, data compression software has transformed into a resource-intensive CPU task. To tackle this challenge, numerous prior studies have introduced hardware acceleration methods. For example, they have utilized Content-Addressable Memory (CAM) for string matches, incorporated redundant historical copies for each matching component, and so on. While these methods amplify the compression throughput, they often compromise an essential aspect of compression performance: the compression ratio (C.R.). Moreover, hardware accelerators face significant resource costs, especially in memory, when dealing with new large sliding window algorithms.We introduce BeeZip, the first hardware acceleration system designed explicitly for compression with a large sliding window. BeeZip tackles the hardware-level challenge of optimizing both compression ratio and throughput. BeeZip offers architectural support for compression algorithms with the following distinctive attributes: 1) A two-stage compression algorithm adapted for accelerator parallelism, decoupling hash parallelism and match execution dependencies; 2) An organized hash hardware accelerator named BeeHash engine enhanced with dynamic scheduling, which orchestrates hash processes with structured parallelism; 3) A hardware accelerator named HiveMatch engine for the match process, which employs a new scalable parallelism approach and a heterogeneous scale processing unit design to reduce memory resource overhead.Experimental results show that on the Silesia dataset, BeeZip achieves an optimal throughput of 10.42GB/s (C.R. 2.96) and the best C.R. of 3.14 (throughput of 5.95GB/s). Under similar compression ratios, compared to single-threaded/36-threaded software implementations, BeeZip offers accelerator speedups of 23.2\texttimes{}/2.45\texttimes{}, respectively. Against all accelerators we know, BeeZip consistently demonstrates a superior compression ratio, improving by at least 9\%.},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages     = {133–148},
  numpages  = {16},
  location  = {La Jolla, CA, USA},
  series    = {ASPLOS '24}
}
@inproceedings{patel2023xfm,
  author     = {Patel, Neel and Mamandipoor, Amin and Quinn, Derrick and Alian, Mohammad},
  title      = {XFM: Accelerated Software-Defined Far Memory},
  pages      = {769--783},
  year       = {2023},
  url        = {https://dl.acm.org/doi/10.1145/3613424.3623776},
  urldate    = {2025-01-06},
  shorttitle = {XFM},
  booktitle  = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
  series     = {MICRO '23},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  isbn       = {979-8-4007-0329-4},
  language   = {en}
}

@inproceedings{9860164,
  author    = {Lian, Zhilu and Li, Yangzi and Chen, Zhixiang and Shan, Shiwen and Han, Baoxin and Su, Yuxin},
  booktitle = {2022 International Conference on Service Science (ICSS)},
  title     = {eBPF-based Working Set Size Estimation in Memory Management},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {188-195},
  keywords  = {Memory management;Estimation;Predictive models;Information filters;Virtual machining;Data models;Resource management;eBPF;Working Set Size Estimation;LightGBM},
  doi       = {10.1109/ICSS55994.2022.00036}
}

@article{mutlu2013memory,
  author     = {Mutlu, Onur},
  title      = {Memory Scaling: A Systems Architecture Perspective},
  journal    = {2013 5th IEEE International Memory Workshop},
  pages      = {21-25},
  year       = {2013},
  url        = {http://ieeexplore.ieee.org/document/6582088/},
  urldate    = {2025-02-10},
  shorttitle = {Memory Scaling},
  publisher  = {IEEE},
  location   = {Monterey, CA, USA},
  isbn       = {9781467361699 9781467361682},
  booktitle  = {2013 5th IEEE International Memory Workshop (IMW)}
}
@article{mutlu2014research,
  author  = {Mutlu, Onur and Subramanian, Lavanya},
  title   = {Research Problems and Opportunities in Memory Systems},
  journal = {Supercomput. Front. Innov.: Int. J.},
  volume  = {1},
  number  = {3},
  pages   = {19--55},
  year    = {2014},
  issn    = {2409-6008},
  url     = {https://doi.org/10.14529/jsfi140302},
  urldate = {2025-02-10}
}
@article{10.1145/356571.356573,
  author     = {Denning, Peter J.},
  title      = {Virtual Memory},
  year       = {1970},
  issue_date = {Sept. 1970},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {2},
  number     = {3},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/356571.356573},
  doi        = {10.1145/356571.356573},
  journal    = {ACM Comput. Surv.},
  month      = sep,
  pages      = {153–189},
  numpages   = {37}
}
@inproceedings{10.1145/1476793.1476834,
  author    = {Weizer, Norman and Oppenheimer, G.},
  title     = {Virtual memory management in a paging environment},
  year      = {1969},
  isbn      = {9781450379021},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1476793.1476834},
  doi       = {10.1145/1476793.1476834},
  abstract  = {The Spectra 70/46 Time Sharing Operating System (TSOS) is designed to be a combined time-sharing and multiprogramming system that will support up to 48 conversational users or a combined total of 64 batch and interactive tasks processing simultaneously.},
  booktitle = {Proceedings of the May 14-16, 1969, Spring Joint Computer Conference},
  pages     = {249–256},
  numpages  = {8},
  location  = {Boston, Massachusetts},
  series    = {AFIPS '69 (Spring)}
}
@inproceedings{10.1145/1555754.1555760,
  author    = {Qureshi, Moinuddin K. and Srinivasan, Vijayalakshmi and Rivers, Jude A.},
  title     = {Scalable high performance main memory system using phase-change memory technology},
  year      = {2009},
  isbn      = {9781605585260},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1555754.1555760},
  doi       = {10.1145/1555754.1555760},
  booktitle = {Proceedings of the 36th Annual International Symposium on Computer Architecture},
  pages     = {24–33},
  numpages  = {10},
  keywords  = {dram caching, phase change memory, wear leveling},
  location  = {Austin, TX, USA},
  series    = {ISCA '09}
}

@article{10.1145/1555815.1555760,
  author     = {Qureshi, Moinuddin K. and Srinivasan, Vijayalakshmi and Rivers, Jude A.},
  title      = {Scalable high performance main memory system using phase-change memory technology},
  year       = {2009},
  issue_date = {June 2009},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {37},
  number     = {3},
  issn       = {0163-5964},
  url        = {https://doi.org/10.1145/1555815.1555760},
  doi        = {10.1145/1555815.1555760},
  journal    = {SIGARCH Comput. Archit. News},
  month      = jun,
  pages      = {24–33},
  numpages   = {10},
  keywords   = {dram caching, phase change memory, wear leveling}
}
@article{wang2024nvpc,
  title   = {NVPC: A Transparent NVM Page Cache},
  author  = {Wang, Guoyu and Che, Xilong and Wei, Haoyang and Chen, Shuo and He, Puyi and Hu, Juncheng},
  journal = {arXiv preprint arXiv:2408.02911},
  year    = {2024}
}
@inproceedings{7304362,
  author    = {Lindstrom, Jan and Das, Dhananjoy and Mathiasen, Torben and Arteaga, Dulcardo and Talagala, Nisha},
  booktitle = {2015 IEEE Non-Volatile Memory System and Applications Symposium (NVMSA)},
  title     = {NVM aware MariaDB database system},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {1-6},
  keywords  = {Databases;Nonvolatile memory;Performance evaluation;Benchmark testing;Ash;Message systems;Engines},
  doi       = {10.1109/NVMSA.2015.7304362}
}
@inproceedings{10.1145/3183713.3196897,
  author    = {van Renen, Alexander and Leis, Viktor and Kemper, Alfons and Neumann, Thomas and Hashida, Takushi and Oe, Kazuichi and Doi, Yoshiyasu and Harada, Lilian and Sato, Mitsuru},
  title     = {Managing Non-Volatile Memory in Database Systems},
  year      = {2018},
  isbn      = {9781450347037},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3183713.3196897},
  doi       = {10.1145/3183713.3196897},
  booktitle = {Proceedings of the 2018 International Conference on Management of Data},
  pages     = {1541–1555},
  numpages  = {15},
  keywords  = {non-volatile memory, database architecture},
  location  = {Houston, TX, USA},
  series    = {SIGMOD '18}
}
@article{debrabant2014prolegomenon,
  title   = {A prolegomenon on OLTP database systems for non-volatile memory},
  author  = {DeBrabant, Justin and Arulraj, Joy and Pavlo, Andrew and Stonebraker, Michael and Zdonik, Stan and Dulloor, Subramanya},
  journal = {ADMS@ VLDB},
  year    = {2014}
}
@inproceedings{DeBrabant2014APO,
  added-at  = {2021 0200},
  author    = {DeBrabant, Justin A. and Arulraj, Joy and Pavlo, Andrew and Stonebraker, Michael and Zdonik, Stanley B. and Dulloor, Subramanya},
  biburl    = {https://www.bibsonomy.org/bibtex/2e5612b9b80599b7131046f1d5b72af92/dblp},
  booktitle = {ADMS@VLDB},
  editor    = {Bordawekar, Rajesh and Lahiri, Tirthankar and Gedik, Bugra and Lang, Christian A.},
  ee        = {http://www.adms-conf.org/2014/adms14_debrabant.pdf},
  interhash = {e1b02141d670ef9628dd969d8e85a0d3},
  intrahash = {e5612b9b80599b7131046f1d5b72af92},
  keywords  = {dblp},
  pages     = {57-63},
  timestamp = {2024-04-10T20:00:55.000+0200},
  title     = {A Prolegomenon on OLTP Database Systems for Non-Volatile Memory},
  url       = {http://dblp.uni-trier.de/db/conf/vldb/adms2014.html#DeBrabantAPSZD14},
  year      = 2014
}

@inproceedings{10.1145/3132402.3132409,
  author    = {Fedorov, Viacheslav and Kim, Jinchun and Qin, Mian and Gratz, Paul V. and Reddy, A. L. Narasimha},
  title     = {Speculative paging for future NVM storage},
  year      = {2017},
  isbn      = {9781450353359},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3132402.3132409},
  doi       = {10.1145/3132402.3132409},
  booktitle = {Proceedings of the International Symposium on Memory Systems},
  pages     = {399–410},
  numpages  = {12},
  keywords  = {memory system, paging, prefetching},
  location  = {Alexandria, Virginia},
  series    = {MEMSYS '17}
}
@inproceedings{Denning1968ThrashingIC,
  author    = {Denning, Peter J.},
  title     = {Thrashing: its causes and prevention},
  year      = {1968},
  isbn      = {9781450378994},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1476589.1476705},
  doi       = {10.1145/1476589.1476705},
  booktitle = {Managing Requirements Knowledge, International Workshop on},
  pages     = {915–922},
  numpages  = {8},
  location  = {San Francisco, California},
  series    = {AFIPS '68 (Fall, part I)}
}


@article{6770405,
  author   = {Thompson, K.},
  journal  = {The Bell System Technical Journal},
  title    = {UNIX time-sharing system: UNIX implementation},
  year     = {1978},
  volume   = {57},
  number   = {6},
  pages    = {1931-1946},
  keywords = {},
  doi      = {10.1002/j.1538-7305.1978.tb02137.x}
}

@article{Mutlu2013MemorySA,
  title   = {Memory scaling: A systems architecture perspective},
  author  = {Onur Mutlu},
  journal = {2013 5th IEEE International Memory Workshop},
  year    = {2013},
  pages   = {21-25},
  url     = {https://api.semanticscholar.org/CorpusID:8098269}
}
@inproceedings{reiss2012heterogeneity,
  author     = {Reiss, Charles and Tumanov, Alexey and Ganger, Gregory R. and Katz, Randy H. and Kozuch, Michael A.},
  title      = {Heterogeneity and Dynamicity of Clouds at Scale: Google Trace Analysis},
  pages      = {1--13},
  year       = {2012},
  url        = {https://dl.acm.org/doi/10.1145/2391229.2391236},
  urldate    = {2025-01-19},
  shorttitle = {Heterogeneity and Dynamicity of Clouds at Scale},
  booktitle  = {Proceedings of the Third ACM Symposium on Cloud Computing},
  series     = {SoCC '12},
  publisher  = {Association for Computing Machinery},
  location   = {New York, NY, USA},
  isbn       = {978-1-4503-1761-0}
}

@conference{liuxf2006,
  author    = {Liu, X F and Wang, Bing Zhong and Shao, Wei and Wen Wang},
  title     = {A marching-on-in-order scheme for exact attenuation constant extraction of lossy transmission lines},
  year      = {2006},
  pages     = {527-529},
  address   = {Chengdu},
  booktitle = {China-Japan Joint Microwave Conference Proceedings}
}


@phdthesis{喻明2023面向NVM和SSD的列存数据库存储引擎设计与实现,
  title       = {面向NVM和SSD的列存数据库存储引擎设计与实现},
  author      = {喻明},
  institution = {华东师范大学},
  year        = {2023},
  pages       = {100-101},
  address     = {上海}
}

@phdthesis{董创轼2023基于NVM的数据库存储引擎优化技术研究,
  title       = {基于NVM的数据库存储引擎优化技术研究},
  author      = {董创轼},
  institution = {电子科技大学},
  year        = {2023},
  pages       = {100-101},
  address     = {成都}
}

@phdthesis{李心池2018基于NVM的内存数据库多表连接操作的设计与优化,
  title       = {基于NVM的内存数据库多表连接操作的设计与优化},
  author      = {李心池},
  institution = {重庆大学},
  year        = {2018},
  pages       = {100-101},
  address     = {重庆}
}
@article{8964479,
  author   = {Cao, Wenqi and Liu, Ling},
  journal  = {IEEE Transactions on Computers},
  title    = {Hierarchical Orchestration of Disaggregated Memory},
  year     = {2020},
  volume   = {69},
  number   = {6},
  pages    = {844-855},
  keywords = {Memory management;Resource management;Throughput;Transient analysis;Delays;Cloud computing;Proposals;Memory disaggregation;virtualization;operating system},
  doi      = {10.1109/TC.2020.2968525}
}
@article{mutlu2019processing,
  author     = {Mutlu, Onur and Ghose, Saugata and G\'{o}mez-Luna, Juan and Ausavarungnirun, Rachata},
  title      = {Processing data where it makes sense: Enabling in-memory computation},
  year       = {2019},
  issue_date = {Jun 2019},
  publisher  = {Elsevier Science Publishers B. V.},
  address    = {NLD},
  volume     = {67},
  number     = {C},
  issn       = {0141-9331},
  url        = {https://doi.org/10.1016/j.micpro.2019.01.009},
  doi        = {10.1016/j.micpro.2019.01.009},
  journal    = {Microprocess. Microsyst.},
  month      = jun,
  pages      = {28–41},
  numpages   = {14},
  keywords   = {Data movement, Main memory, Processing-in-memory, 3D-Stacked memory, Near-data processing}
}
@inproceedings{hemem2021,
  author    = {Raybuck, Amanda and Stamler, Tim and Zhang, Wei and Erez, Mattan and Peter, Simon},
  title     = {HeMem: Scalable Tiered Memory Management for Big Data Applications and Real NVM},
  year      = {2021},
  isbn      = {9781450387095},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3477132.3483550},
  doi       = {10.1145/3477132.3483550},
  abstract  = {High-capacity non-volatile memory (NVM) is a new main memory tier. Tiered DRAM+NVM servers increase total memory capacity by up to 8x, but can diminish memory bandwidth by up to 7x and inflate latency by up to 63\% if not managed well. We study existing hardware and software tiered memory management systems on the recently available Intel Optane DC NVM with big data applications and find that no existing system maximizes application performance on real NVM.Based on our findings, we present HeMem, a tiered main memory management system designed from scratch for commercially available NVM and the big data applications that use it. HeMem manages tiered memory asynchronously, batching and amortizing memory access tracking, migration, and associated TLB synchronization overheads. HeMem monitors application memory use by sampling memory access via CPU events, rather than page tables. This allows HeMem to scale to terabytes of memory, keeping small and ephemeral data structures in fast memory, and allocating scarce, asymmetric NVM bandwidth according to access patterns. Finally, HeMem is flexible by placing per-application memory management policy at user-level. On a system with Intel Optane DC NVM, HeMem outperforms hardware, OS, and PL-based tiered memory management, providing up to 50\% runtime reduction for the GAP graph processing benchmark, 13\% higher throughput for TPC-C on the Silo in-memory database, 16\% lower tail-latency under performance isolation for a key-value store, and up to 10x less NVM wear than the next best solution, without application modification.},
  booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
  pages     = {392–407},
  numpages  = {16},
  keywords  = {Operating system, Scalability, Tiered memory management},
  location  = {Virtual Event, Germany},
  series    = {SOSP '21}
}
@inproceedings{Zhong2024ManagingMT,
  author    = {Zhong, Yuhong and Berger, Daniel S. and Waldspurger, Carl and Wee, Ryan and Agarwal, Ishwar and Agarwal, Rajat and Hady, Frank and Kumar, Karthik and Hill, Mark D. and Chowdhury, Mosharaf and Cidon, Asaf},
  title     = {Managing memory tiers with CXL in virtualized environments},
  year      = {2024},
  isbn      = {978-1-939133-40-3},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {Cloud providers seek to deploy CXL-based memory to increase aggregate memory capacity, reduce costs, and lower carbon emissions. However, CXL accesses incur higher latency than local DRAM. Existing systems use software to manage data placement across memory tiers at page granularity. Cloud providers are reluctant to deploy software-based tiering due to high overheads in virtualized environments. Hardware-based memory tiering could place data at cacheline granularity, mitigating these drawbacks. However, hardware is oblivious to application-level performance.We propose combining hardware-managed tiering with software-managed performance isolation to overcome the pitfalls of either approach. We introduce Intel® Flat Memory Mode, the first hardware-managed tiering system for CXL. Our evaluation on a full-system prototype demonstrates that it provides performance close to regular DRAM, with no more than 5\% degradation for more than 82\% of workloads. Despite such small slowdowns, we identify two challenges that can still degrade performance by up to 34\% for "outlier" workloads: (1) memory contention across tenants, and (2) intra-tenant contention due to conflicting access patterns.To address these challenges, we introduce Memstrata, a lightweight multi-tenant memory allocator. Memstrata employs page coloring to eliminate inter-VM contention. It improves performance for VMs with access patterns that are sensitive to hardware tiering by allocating them more local DRAM using an online slowdown estimator. In multi-VM experiments on prototype hardware, Memstrata is able to identify performance outliers and reduce their degradation from above 30\% to below 6\%, providing consistent performance across a wide range of workloads.},
  booktitle = {Proceedings of the 18th USENIX Conference on Operating Systems Design and Implementation},
  articleno = {3},
  numpages  = {20},
  location  = {Santa Clara, CA, USA},
  series    = {OSDI'24},
  pages     = {3-22}
}

@thesis{HuangJiYuQueYeYiChangDeFenB,
  author      = {黄希光},
  title       = {基于缺页异常的分布式内存管理系统设计与实现},
  school      = {电子科技大学},
  year        = {2022},
  type        = {硕士论文},
  institution = {电子科技大学},
  address     = {成都},
  month       = {},
  pages       = {1-77}
}
@inproceedings{201565,
  author    = {Juncheng Gu and Youngmoon Lee and Yiwen Zhang and Mosharaf Chowdhury and Kang G. Shin},
  title     = {Efficient Memory Disaggregation with Infiniswap},
  booktitle = {14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)},
  year      = {2017},
  isbn      = {978-1-931971-37-9},
  address   = {Boston, MA},
  pages     = {649--667},
  url       = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/gu},
  publisher = {USENIX Association},
  month     = mar
}


@inproceedings{10.1145/3297858.3304053,
  author    = {Lagar-Cavilla, Andres and Ahn, Junwhan and Souhlal, Suleiman and Agarwal, Neha and Burny, Radoslaw and Butt, Shakeel and Chang, Jichuan and Chaugule, Ashwin and Deng, Nan and Shahid, Junaid and Thelen, Greg and Yurtsever, Kamil Adam and Zhao, Yu and Ranganathan, Parthasarathy},
  title     = {Software-Defined Far Memory in Warehouse-Scale Computers},
  year      = {2019},
  isbn      = {9781450362405},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3297858.3304053},
  doi       = {10.1145/3297858.3304053},
  booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages     = {317–330},
  numpages  = {14},
  keywords  = {cold data, far memory, machine learning, memory, warehouse-scale computers, zswap},
  location  = {Providence, RI, USA},
  series    = {ASPLOS '19}
}


@article{9076292,
  author   = {Xie, Yulai and Jin, Minpeng and Zou, Zhuping and Xu, Gongming and Feng, Dan and Liu, Wenmao and Long, Darrell},
  journal  = {IEEE Transactions on Cloud Computing},
  title    = {Real-Time Prediction of Docker Container Resource Load Based on a Hybrid Model of ARIMA and Triple Exponential Smoothing},
  year     = {2022},
  volume   = {10},
  number   = {2},
  pages    = {1386-1401},
  keywords = {Containers;Predictive models;Load modeling;Smoothing methods;Cloud computing;Time series analysis;Computational modeling;Docker container;prediction;hybrid model},
  doi      = {10.1109/TCC.2020.2989631}
}
@article{9870561,
  author   = {Ding, Zhijun and Feng, Binbin and Jiang, Changjun},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  title    = {COIN: A Container Workload Prediction Model Focusing on Common and Individual Changes in Workloads},
  year     = {2022},
  volume   = {33},
  number   = {12},
  pages    = {4738-4751},
  keywords = {Containers;Predictive models;Data models;Adaptation models;Forecasting;Cloud computing;Load modeling;Cloud Computing;container;workload prediction;container similarity;online learning;transfer learning;integrated model},
  doi      = {10.1109/TPDS.2022.3202833}
}
@article{10.1155/2023/5959223,
  author     = {Wang, Lu and Guo, Shuaidong and Zhang, Pengli and Yue, Haodong and Li, Yaxiao and Wang, Chenyi and Cao, Zhuang and Cui, Di and Khosravi, Mohammad R.},
  title      = {An Efficient Load Prediction-Driven Scheduling Strategy Model in Container Cloud},
  year       = {2023},
  issue_date = {2023},
  publisher  = {John Wiley and Sons Ltd.},
  address    = {GBR},
  volume     = {2023},
  issn       = {0884-8173},
  url        = {https://doi.org/10.1155/2023/5959223},
  doi        = {10.1155/2023/5959223},
  journal    = {Int. J. Intell. Syst.},
  pages      = {1-25},
  month      = jan,
  numpages   = {25}
}
@inproceedings{Melekhova2015EstimatingWS,
  title     = {Estimating Working Set Size by Guest OS Performance Counters Means},
  author    = {Anna Melekhova and Larisa Markeeva},
  booktitle = {IEEE CLOUD 2015},
  year      = {2015},
  url       = {https://api.semanticscholar.org/CorpusID:55162414},
  pages     = {33-38}
}

@inproceedings{10.1145/945445.945462,
  author    = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
  title     = {Xen and the art of virtualization},
  year      = {2003},
  isbn      = {1581137575},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/945445.945462},
  doi       = {10.1145/945445.945462},
  abstract  = {Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100\% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead --- at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.},
  booktitle = {Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles},
  pages     = {164–177},
  numpages  = {14},
  keywords  = {virtual machine monitors, paravirtualization, hypervisors},
  location  = {Bolton Landing, NY, USA},
  series    = {SOSP '03}
}

@article{10.1145/1165389.945462,
  author     = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
  title      = {Xen and the art of virtualization},
  year       = {2003},
  issue_date = {December 2003},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {37},
  number     = {5},
  issn       = {0163-5980},
  url        = {https://doi.org/10.1145/1165389.945462},
  doi        = {10.1145/1165389.945462},
  abstract   = {Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100\% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead --- at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.},
  journal    = {SIGOPS Oper. Syst. Rev.},
  month      = oct,
  pages      = {164–177},
  numpages   = {14},
  keywords   = {virtual machine monitors, paravirtualization, hypervisors}
}


@article{10.1145/3179422,
  author     = {Nitu, Vlad and Kocharyan, Aram and Yaya, Hannas and Tchana, Alain and Hagimont, Daniel and Astsatryan, Hrachya},
  title      = {Working Set Size Estimation Techniques in Virtualized Environments: One Size Does not Fit All},
  year       = {2018},
  issue_date = {March 2018},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {2},
  number     = {1},
  url        = {https://doi.org/10.1145/3179422},
  doi        = {10.1145/3179422},
  journal    = {Proc. ACM Meas. Anal. Comput. Syst.},
  month      = apr,
  articleno  = {19},
  numpages   = {22},
  keywords   = {virtualization, energy consumption optimization, cloud computing}
}
@book{zhu1973wulixue,
  title     = {物理学},
  author    = {竺可桢},
  year      = {1973},
  address   = {北京},
  pages     = {56-60},
  publisher = {科学出版社}
}

@thesis{chen2001hao,
  author      = {陈念永},
  title       = {毫米波细胞生物效应及抗肿瘤研究},
  institution = {电子科技大学},
  year        = {2001},
  pages       = {50-60},
  address     = {成都}
}
@inproceedings{10.1145/3342195.3387522,
  author    = {Amaro, Emmanuel and Branner-Augmon, Christopher and Luo, Zhihong and Ousterhout, Amy and Aguilera, Marcos K. and Panda, Aurojit and Ratnasamy, Sylvia and Shenker, Scott},
  title     = {Can far memory improve job throughput?},
  year      = {2020},
  isbn      = {9781450368827},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3342195.3387522},
  doi       = {10.1145/3342195.3387522},
  booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
  articleno = {14},
  numpages  = {16},
  location  = {Heraklion, Greece},
  series    = {EuroSys '20},
  pages     = {1-16}
}
@newspaper{gu2012lao,
  author  = {顾春},
  title   = {牢牢把握稳中求进的总基调},
  journal = {人民日报},
  date    = {2012年3月31日}
}

@techreport{feng997he,
  author      = {冯西桥},
  title       = {核反应堆压力容器的{LBB}分析},
  institution = {清华大学核能技术设计研究院},
  date        = {1997年6月25日},
  address     = {北京}
}

@patent{xiao2012yi,
  author  = {肖珍新},
  title   = {一种新型排渣阀调节降温装置},
  date    = {2012年4月25日},
  type    = {实用新型专利},
  country = {中国},
  id      = {ZL201120085830.0}
}

@standard{zhong1994zhong,
  institution = {中华人民共和国国家技术监督局},
  id          = {GB3100-3102},
  title       = {中华人民共和国国家标准--量与单位},
  publisher   = {中国标准出版社},
  date        = {1994年11月1日},
  address     = {北京}
}

@digital{clerc2010discrete,
  author = {M. Clerc},
  title  = {Discrete particle swarm optimization: a fuzzy combinatorial box},
  type   = {EB/OL},
  date   = {July 16, 2010},
  url    = {http://clere.maurice.free.fr/pso/Fuzzy_Discrere_PSO/Fuzzy_DPSO.htm}
}
